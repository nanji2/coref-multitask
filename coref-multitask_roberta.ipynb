{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5544d3ec-a275-43af-9704-32cfe4747c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da81ce68-01b4-45ee-96f2-5ccd272d97aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99309099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.legacy import data\n",
    "from torchtext import datasets\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import random\n",
    "import functools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf54be0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters & setup\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "DROPOUT = 0.2\n",
    "N_EPOCHS = 60\n",
    "BATCH_SIZE = 2\n",
    "LEARNING_RATE = 1e-6\n",
    "NO_HEAD_TRANS = 16\n",
    "\n",
    "TAG_LOSS_WEIGTH = 0.8\n",
    "CLS_LOSS_WEIGTH = 1\n",
    "\n",
    "BERT_PATH = './roberta-large' # the path of your downloaded pre-trained language model\n",
    "DATA_PATH = './gap-data/'\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(BERT_PATH)\n",
    "bert = RobertaModel.from_pretrained(BERT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "681d50c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pre-processing\n",
    "\n",
    "def read_token_idx_list_n_cut_to_max_length(tokens, max_input_length):\n",
    "    \n",
    "    tokens =  tokens[:max_input_length-1]\n",
    "    tokens_list = []\n",
    "    for i in tokens:\n",
    "        tokens_list.append(int(i))\n",
    "    return torch.tensor(tokens_list)\n",
    "\n",
    "def cut_to_max_length(tokens, max_input_length):\n",
    "    tokens = tokens[:max_input_length-1]\n",
    "    return tokens\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "pad_token = tokenizer.pad_token\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "max_input_length = tokenizer.max_model_input_sizes['roberta-large']\n",
    "text_id_preprocessor = functools.partial(read_token_idx_list_n_cut_to_max_length,max_input_length = max_input_length)\n",
    "tag_preprocessor = functools.partial(cut_to_max_length, max_input_length = max_input_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bd6ba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "\n",
    "TOKEN = data.Field(batch_first = True)\n",
    "TOKEN_ID = data.Field(batch_first = True,\n",
    "                     use_vocab = False,\n",
    "                     dtype=torch.int64,\n",
    "                     pad_token = pad_token_idx,\n",
    "                     preprocessing = text_id_preprocessor)\n",
    "LABEL1 = data.Field(batch_first = True,\n",
    "                    unk_token = None,\n",
    "                    preprocessing = tag_preprocessor)\n",
    "MASK = data.Field(batch_first = True,\n",
    "                     use_vocab = False,\n",
    "                     dtype=torch.int64,\n",
    "                     pad_token = pad_token_idx,\n",
    "                     preprocessing = text_id_preprocessor)\n",
    "FT_TAGS = data.Field(batch_first = True,\n",
    "                     use_vocab = False,\n",
    "                     dtype=torch.int64,\n",
    "                     pad_token = pad_token_idx,\n",
    "                     preprocessing = text_id_preprocessor)\n",
    "SEQ = data.Field(batch_first = True)\n",
    "LABEL2 = data.Field(batch_first = True,\n",
    "                    unk_token = None,\n",
    "                    preprocessing = tag_preprocessor)\n",
    "\n",
    "fields = ((\"token\", TOKEN),\n",
    "          ('token_id', TOKEN_ID),\n",
    "          ('label1', LABEL1),\n",
    "          ('mask', MASK),\n",
    "          ('first_token', FT_TAGS),\n",
    "          ('seq', SEQ),\n",
    "          ('label2', LABEL2))\n",
    "\n",
    "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
    "                                    path = DATA_PATH,\n",
    "                                    train = 'gap_train_new.csv',\n",
    "                                    validation = 'gap_dev_new.csv',\n",
    "                                    test = 'gap_test_new.csv',\n",
    "                                    format = 'csv',\n",
    "                                    fields = fields,\n",
    "                                    skip_header = True)\n",
    "\n",
    "TOKEN.build_vocab(train_data, valid_data, test_data)\n",
    "LABEL1.build_vocab(train_data, valid_data, test_data)\n",
    "FT_TAGS.build_vocab(train_data, valid_data, test_data)\n",
    "SEQ.build_vocab(train_data, valid_data, test_data)\n",
    "LABEL2.build_vocab(train_data, valid_data, test_data)\n",
    "\n",
    "# if you want to prepare a big vocabulary that covers words that never appear in your dataset:\n",
    "\n",
    "#word_list = [['<unk>', '<pad>', 'I', 'great', \"it's\", 'like', 'swimming', '.', ',', 'BBBBBBB']]\n",
    "#TOKEN.build_vocab(word_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09b8cde6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: {'token': ['<s>', 'Z', 'oe', 'ĠTel', 'ford', 'Ġ--', 'Ġplayed', 'Ġthe', 'Ġpolice', 'Ġofficer', 'Ġgirlfriend', 'Ġof', 'ĠSimon', ',', 'ĠMaggie', '.', 'ĠD', 'umped', 'Ġby', 'ĠSimon', 'Ġin', 'Ġthe', 'Ġfinal', 'Ġepisode', 'Ġof', 'Ġseries', 'Ġ1', ',', 'Ġafter', 'Ġhe', 'Ġslept', 'Ġwith', 'ĠJenny', ',', 'Ġand', 'Ġis', 'Ġnot', 'Ġseen', 'Ġagain', '.', 'ĠPh', 'oe', 'be', 'ĠThomas', 'Ġplayed', 'ĠCheryl', 'ĠCassidy', ',', 'ĠPaul', 'ine', \"'s\", 'Ġfriend', 'Ġand', 'Ġalso', 'Ġa', 'Ġyear', 'Ġ11', 'Ġpupil', 'Ġin', 'ĠSimon', \"'s\", 'Ġclass', '.', 'ĠD', 'umped', 'Ġher', 'Ġboyfriend', 'Ġfollowing', 'ĠSimon', \"'s\", 'Ġadvice', 'Ġafter', 'Ġhe', 'Ġwouldn', \"'t\", 'Ġhave', 'Ġsex', 'Ġwith', 'Ġher', 'Ġbut', 'Ġlater', 'Ġrealised', 'Ġthis', 'Ġwas', 'Ġdue', 'Ġto', 'Ġhim', 'Ġcatching', 'Ġcrabs', 'Ġoff', 'Ġher', 'Ġfriend', 'ĠPaul', 'ine', '.', '</s>'], 'token_id': tensor([    0,  1301,  3540,  5477,  1891,   480,   702,     5,   249,  1036,\n",
      "         6096,     9,  4616,     6, 18320,     4,   211, 25844,    30,  4616,\n",
      "           11,     5,   507,  3238,     9,   651,   112,     6,    71,    37,\n",
      "        17931,    19, 17016,     6,     8,    16,    45,   450,   456,     4,\n",
      "         4129,  3540,  1610,  1813,   702, 16252, 20194,     6,  1206,   833,\n",
      "           18,  1441,     8,    67,    10,    76,   365, 25209,    11,  4616,\n",
      "           18,  1380,     4,   211, 25844,    69,  6578,   511,  4616,    18,\n",
      "         2949,    71,    37,  1979,    75,    33,  2099,    19,    69,    53,\n",
      "          423, 11555,    42,    21,   528,     7,   123, 10205, 37669,   160,\n",
      "           69,  1441,  1206,   833,     4,     2]), 'label1': ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '1', '1', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], 'mask': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'first_token': tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,\n",
      "        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
      "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]), 'seq': ['Zoe', 'Telford', '--', 'played', 'the', 'police', 'officer', 'girlfriend', 'of', 'Simon,', 'Maggie.', 'Dumped', 'by', 'Simon', 'in', 'the', 'final', 'episode', 'of', 'series', '1,', 'after', 'he', 'slept', 'with', 'Jenny,', 'and', 'is', 'not', 'seen', 'again.', 'Phoebe', 'Thomas', 'played', 'Cheryl', 'Cassidy,', \"Pauline's\", 'friend', 'and', 'also', 'a', 'year', '11', 'pupil', 'in', \"Simon's\", 'class.', 'Dumped', 'her', 'boyfriend', 'following', \"Simon's\", 'advice', 'after', 'he', \"wouldn't\", 'have', 'sex', 'with', 'her', 'but', 'later', 'realised', 'this', 'was', 'due', 'to', 'him', 'catching', 'crabs', 'off', 'her', 'friend', 'Pauline.'], 'label2': ['0']}\n",
      "Label1 vocab ['<pad>', '0', '1']\n",
      "Label2 vocab ['<pad>', '0', '1']\n"
     ]
    }
   ],
   "source": [
    "print('Example:', vars(train_data.examples[2]))\n",
    "print('Label1 vocab', LABEL1.vocab.itos)\n",
    "print('Label2 vocab', LABEL2.vocab.itos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5d813a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device,\n",
    "    shuffle = True,\n",
    "    sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "868bef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTL(nn.Module):\n",
    "    def __init__(self,bert,label1_output_dim,label2_output_dim,dropout,no_head_trans):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.tag_layer1 = nn.TransformerEncoderLayer(d_model = embedding_dim, nhead = no_head_trans)\n",
    "        self.cls_layer1 = nn.TransformerEncoderLayer(d_model = embedding_dim, nhead = no_head_trans)\n",
    "        self.tag_layer2 = nn.TransformerEncoderLayer(d_model = embedding_dim, nhead = no_head_trans)\n",
    "        self.cls_layer2 = nn.TransformerEncoderLayer(d_model = embedding_dim, nhead = no_head_trans)\n",
    "        # self.tag_layer3 = nn.TransformerEncoderLayer(d_model = embedding_dim, nhead = no_head_trans)\n",
    "        # self.cls_layer3 = nn.TransformerEncoderLayer(d_model = embedding_dim, nhead = no_head_trans)\n",
    "        # self.tag_layer4 = nn.TransformerEncoderLayer(d_model = embedding_dim, nhead = no_head_trans)\n",
    "        # self.cls_layer4 = nn.TransformerEncoderLayer(d_model = embedding_dim, nhead = no_head_trans)\n",
    "        \n",
    "        self.fc_tag = nn.Linear(embedding_dim, label1_output_dim)\n",
    "        self.fc_cls = nn.Linear(embedding_dim, label2_output_dim)\n",
    "        \n",
    "    def forward(self, token_id, mask):\n",
    "        \n",
    "        emb_share = self.dropout(self.bert(token_id)[0]) # [batch size, seq len, emb dim]\n",
    "        \n",
    "        tag1 = self.dropout(self.tag_layer1(emb_share))  # [batch size, seq len, emb dim]\n",
    "        cls1 = self.dropout(self.cls_layer1(emb_share))    # [batch size, seq len, emb dim]\n",
    "        tag2 = self.dropout(self.tag_layer2(tag1))  # [batch size, seq len, emb dim]\n",
    "        cls2 = self.dropout(self.cls_layer2(cls1))    # [batch size, seq len, emb dim]\n",
    "        # tag3 = self.dropout(self.tag_layer3(tag2))  # [batch size, seq len, emb dim]\n",
    "        # cls3 = self.dropout(self.cls_layer3(cls2))    # [batch size, seq len, emb dim]\n",
    "        # tag4 = self.dropout(self.tag_layer4(tag2))  # [batch size, seq len, emb dim]\n",
    "        # cls4 = self.dropout(self.cls_layer4(cls3))    # [batch size, seq len, emb dim]\n",
    "        \n",
    "        tag_pred = self.fc_tag(tag2) # [batch size, seq len, output dim]\n",
    "        cls_mask_out = torch.sum(cls2*mask.unsqueeze(2),1) # [batch size, output dim]\n",
    "        cls_pred = self.fc_cls(cls_mask_out)\n",
    "        \n",
    "        return tag_pred, cls_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2f2566a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "The model has 388,965,382 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIM_TAGGING = len(LABEL1.vocab)\n",
    "OUTPUT_DIM_CLASSIFICATION = len(LABEL2.vocab)\n",
    "\n",
    "print(OUTPUT_DIM_TAGGING)\n",
    "print(OUTPUT_DIM_CLASSIFICATION)\n",
    "\n",
    "model = MTL(bert,\n",
    "            OUTPUT_DIM_TAGGING, \n",
    "            OUTPUT_DIM_CLASSIFICATION, \n",
    "            DROPOUT,\n",
    "            NO_HEAD_TRANS)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cff96df-2b10-45dd-835d-8ab3cbeb316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_params = []\n",
    "other_params = []\n",
    "for name, param in model.named_parameters():\n",
    "  if name.startswith(\"bert\"):\n",
    "    bert_params.append(param)\n",
    "  else:\n",
    "    other_params.append(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88a34620-9de1-4681-974a-4466b7bdc3b6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "391"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66678321-1cb9-4e0c-bb8e-5ac474707dd4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(other_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49fb45e3-6f70-420a-8f43-a216a9c0fc18",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight\n",
      "bert.embeddings.position_embeddings.weight\n",
      "bert.embeddings.token_type_embeddings.weight\n",
      "bert.embeddings.LayerNorm.weight\n",
      "bert.embeddings.LayerNorm.bias\n",
      "bert.encoder.layer.0.attention.self.query.weight\n",
      "bert.encoder.layer.0.attention.self.query.bias\n",
      "bert.encoder.layer.0.attention.self.key.weight\n",
      "bert.encoder.layer.0.attention.self.key.bias\n",
      "bert.encoder.layer.0.attention.self.value.weight\n",
      "bert.encoder.layer.0.attention.self.value.bias\n",
      "bert.encoder.layer.0.attention.output.dense.weight\n",
      "bert.encoder.layer.0.attention.output.dense.bias\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.0.intermediate.dense.weight\n",
      "bert.encoder.layer.0.intermediate.dense.bias\n",
      "bert.encoder.layer.0.output.dense.weight\n",
      "bert.encoder.layer.0.output.dense.bias\n",
      "bert.encoder.layer.0.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.attention.self.query.weight\n",
      "bert.encoder.layer.1.attention.self.query.bias\n",
      "bert.encoder.layer.1.attention.self.key.weight\n",
      "bert.encoder.layer.1.attention.self.key.bias\n",
      "bert.encoder.layer.1.attention.self.value.weight\n",
      "bert.encoder.layer.1.attention.self.value.bias\n",
      "bert.encoder.layer.1.attention.output.dense.weight\n",
      "bert.encoder.layer.1.attention.output.dense.bias\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.intermediate.dense.weight\n",
      "bert.encoder.layer.1.intermediate.dense.bias\n",
      "bert.encoder.layer.1.output.dense.weight\n",
      "bert.encoder.layer.1.output.dense.bias\n",
      "bert.encoder.layer.1.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.attention.self.query.weight\n",
      "bert.encoder.layer.2.attention.self.query.bias\n",
      "bert.encoder.layer.2.attention.self.key.weight\n",
      "bert.encoder.layer.2.attention.self.key.bias\n",
      "bert.encoder.layer.2.attention.self.value.weight\n",
      "bert.encoder.layer.2.attention.self.value.bias\n",
      "bert.encoder.layer.2.attention.output.dense.weight\n",
      "bert.encoder.layer.2.attention.output.dense.bias\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.intermediate.dense.weight\n",
      "bert.encoder.layer.2.intermediate.dense.bias\n",
      "bert.encoder.layer.2.output.dense.weight\n",
      "bert.encoder.layer.2.output.dense.bias\n",
      "bert.encoder.layer.2.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.attention.self.query.weight\n",
      "bert.encoder.layer.3.attention.self.query.bias\n",
      "bert.encoder.layer.3.attention.self.key.weight\n",
      "bert.encoder.layer.3.attention.self.key.bias\n",
      "bert.encoder.layer.3.attention.self.value.weight\n",
      "bert.encoder.layer.3.attention.self.value.bias\n",
      "bert.encoder.layer.3.attention.output.dense.weight\n",
      "bert.encoder.layer.3.attention.output.dense.bias\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.intermediate.dense.weight\n",
      "bert.encoder.layer.3.intermediate.dense.bias\n",
      "bert.encoder.layer.3.output.dense.weight\n",
      "bert.encoder.layer.3.output.dense.bias\n",
      "bert.encoder.layer.3.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.attention.self.query.weight\n",
      "bert.encoder.layer.4.attention.self.query.bias\n",
      "bert.encoder.layer.4.attention.self.key.weight\n",
      "bert.encoder.layer.4.attention.self.key.bias\n",
      "bert.encoder.layer.4.attention.self.value.weight\n",
      "bert.encoder.layer.4.attention.self.value.bias\n",
      "bert.encoder.layer.4.attention.output.dense.weight\n",
      "bert.encoder.layer.4.attention.output.dense.bias\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.intermediate.dense.weight\n",
      "bert.encoder.layer.4.intermediate.dense.bias\n",
      "bert.encoder.layer.4.output.dense.weight\n",
      "bert.encoder.layer.4.output.dense.bias\n",
      "bert.encoder.layer.4.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.attention.self.query.weight\n",
      "bert.encoder.layer.5.attention.self.query.bias\n",
      "bert.encoder.layer.5.attention.self.key.weight\n",
      "bert.encoder.layer.5.attention.self.key.bias\n",
      "bert.encoder.layer.5.attention.self.value.weight\n",
      "bert.encoder.layer.5.attention.self.value.bias\n",
      "bert.encoder.layer.5.attention.output.dense.weight\n",
      "bert.encoder.layer.5.attention.output.dense.bias\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.intermediate.dense.weight\n",
      "bert.encoder.layer.5.intermediate.dense.bias\n",
      "bert.encoder.layer.5.output.dense.weight\n",
      "bert.encoder.layer.5.output.dense.bias\n",
      "bert.encoder.layer.5.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.attention.self.query.weight\n",
      "bert.encoder.layer.6.attention.self.query.bias\n",
      "bert.encoder.layer.6.attention.self.key.weight\n",
      "bert.encoder.layer.6.attention.self.key.bias\n",
      "bert.encoder.layer.6.attention.self.value.weight\n",
      "bert.encoder.layer.6.attention.self.value.bias\n",
      "bert.encoder.layer.6.attention.output.dense.weight\n",
      "bert.encoder.layer.6.attention.output.dense.bias\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.intermediate.dense.weight\n",
      "bert.encoder.layer.6.intermediate.dense.bias\n",
      "bert.encoder.layer.6.output.dense.weight\n",
      "bert.encoder.layer.6.output.dense.bias\n",
      "bert.encoder.layer.6.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.attention.self.query.weight\n",
      "bert.encoder.layer.7.attention.self.query.bias\n",
      "bert.encoder.layer.7.attention.self.key.weight\n",
      "bert.encoder.layer.7.attention.self.key.bias\n",
      "bert.encoder.layer.7.attention.self.value.weight\n",
      "bert.encoder.layer.7.attention.self.value.bias\n",
      "bert.encoder.layer.7.attention.output.dense.weight\n",
      "bert.encoder.layer.7.attention.output.dense.bias\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.intermediate.dense.weight\n",
      "bert.encoder.layer.7.intermediate.dense.bias\n",
      "bert.encoder.layer.7.output.dense.weight\n",
      "bert.encoder.layer.7.output.dense.bias\n",
      "bert.encoder.layer.7.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.attention.self.query.weight\n",
      "bert.encoder.layer.8.attention.self.query.bias\n",
      "bert.encoder.layer.8.attention.self.key.weight\n",
      "bert.encoder.layer.8.attention.self.key.bias\n",
      "bert.encoder.layer.8.attention.self.value.weight\n",
      "bert.encoder.layer.8.attention.self.value.bias\n",
      "bert.encoder.layer.8.attention.output.dense.weight\n",
      "bert.encoder.layer.8.attention.output.dense.bias\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.intermediate.dense.weight\n",
      "bert.encoder.layer.8.intermediate.dense.bias\n",
      "bert.encoder.layer.8.output.dense.weight\n",
      "bert.encoder.layer.8.output.dense.bias\n",
      "bert.encoder.layer.8.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.attention.self.query.weight\n",
      "bert.encoder.layer.9.attention.self.query.bias\n",
      "bert.encoder.layer.9.attention.self.key.weight\n",
      "bert.encoder.layer.9.attention.self.key.bias\n",
      "bert.encoder.layer.9.attention.self.value.weight\n",
      "bert.encoder.layer.9.attention.self.value.bias\n",
      "bert.encoder.layer.9.attention.output.dense.weight\n",
      "bert.encoder.layer.9.attention.output.dense.bias\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.intermediate.dense.weight\n",
      "bert.encoder.layer.9.intermediate.dense.bias\n",
      "bert.encoder.layer.9.output.dense.weight\n",
      "bert.encoder.layer.9.output.dense.bias\n",
      "bert.encoder.layer.9.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.attention.self.query.weight\n",
      "bert.encoder.layer.10.attention.self.query.bias\n",
      "bert.encoder.layer.10.attention.self.key.weight\n",
      "bert.encoder.layer.10.attention.self.key.bias\n",
      "bert.encoder.layer.10.attention.self.value.weight\n",
      "bert.encoder.layer.10.attention.self.value.bias\n",
      "bert.encoder.layer.10.attention.output.dense.weight\n",
      "bert.encoder.layer.10.attention.output.dense.bias\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.intermediate.dense.weight\n",
      "bert.encoder.layer.10.intermediate.dense.bias\n",
      "bert.encoder.layer.10.output.dense.weight\n",
      "bert.encoder.layer.10.output.dense.bias\n",
      "bert.encoder.layer.10.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.attention.self.query.weight\n",
      "bert.encoder.layer.11.attention.self.query.bias\n",
      "bert.encoder.layer.11.attention.self.key.weight\n",
      "bert.encoder.layer.11.attention.self.key.bias\n",
      "bert.encoder.layer.11.attention.self.value.weight\n",
      "bert.encoder.layer.11.attention.self.value.bias\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "bert.encoder.layer.11.attention.output.dense.bias\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.intermediate.dense.weight\n",
      "bert.encoder.layer.11.intermediate.dense.bias\n",
      "bert.encoder.layer.11.output.dense.weight\n",
      "bert.encoder.layer.11.output.dense.bias\n",
      "bert.encoder.layer.11.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.output.LayerNorm.bias\n",
      "bert.encoder.layer.12.attention.self.query.weight\n",
      "bert.encoder.layer.12.attention.self.query.bias\n",
      "bert.encoder.layer.12.attention.self.key.weight\n",
      "bert.encoder.layer.12.attention.self.key.bias\n",
      "bert.encoder.layer.12.attention.self.value.weight\n",
      "bert.encoder.layer.12.attention.self.value.bias\n",
      "bert.encoder.layer.12.attention.output.dense.weight\n",
      "bert.encoder.layer.12.attention.output.dense.bias\n",
      "bert.encoder.layer.12.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.12.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.12.intermediate.dense.weight\n",
      "bert.encoder.layer.12.intermediate.dense.bias\n",
      "bert.encoder.layer.12.output.dense.weight\n",
      "bert.encoder.layer.12.output.dense.bias\n",
      "bert.encoder.layer.12.output.LayerNorm.weight\n",
      "bert.encoder.layer.12.output.LayerNorm.bias\n",
      "bert.encoder.layer.13.attention.self.query.weight\n",
      "bert.encoder.layer.13.attention.self.query.bias\n",
      "bert.encoder.layer.13.attention.self.key.weight\n",
      "bert.encoder.layer.13.attention.self.key.bias\n",
      "bert.encoder.layer.13.attention.self.value.weight\n",
      "bert.encoder.layer.13.attention.self.value.bias\n",
      "bert.encoder.layer.13.attention.output.dense.weight\n",
      "bert.encoder.layer.13.attention.output.dense.bias\n",
      "bert.encoder.layer.13.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.13.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.13.intermediate.dense.weight\n",
      "bert.encoder.layer.13.intermediate.dense.bias\n",
      "bert.encoder.layer.13.output.dense.weight\n",
      "bert.encoder.layer.13.output.dense.bias\n",
      "bert.encoder.layer.13.output.LayerNorm.weight\n",
      "bert.encoder.layer.13.output.LayerNorm.bias\n",
      "bert.encoder.layer.14.attention.self.query.weight\n",
      "bert.encoder.layer.14.attention.self.query.bias\n",
      "bert.encoder.layer.14.attention.self.key.weight\n",
      "bert.encoder.layer.14.attention.self.key.bias\n",
      "bert.encoder.layer.14.attention.self.value.weight\n",
      "bert.encoder.layer.14.attention.self.value.bias\n",
      "bert.encoder.layer.14.attention.output.dense.weight\n",
      "bert.encoder.layer.14.attention.output.dense.bias\n",
      "bert.encoder.layer.14.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.14.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.14.intermediate.dense.weight\n",
      "bert.encoder.layer.14.intermediate.dense.bias\n",
      "bert.encoder.layer.14.output.dense.weight\n",
      "bert.encoder.layer.14.output.dense.bias\n",
      "bert.encoder.layer.14.output.LayerNorm.weight\n",
      "bert.encoder.layer.14.output.LayerNorm.bias\n",
      "bert.encoder.layer.15.attention.self.query.weight\n",
      "bert.encoder.layer.15.attention.self.query.bias\n",
      "bert.encoder.layer.15.attention.self.key.weight\n",
      "bert.encoder.layer.15.attention.self.key.bias\n",
      "bert.encoder.layer.15.attention.self.value.weight\n",
      "bert.encoder.layer.15.attention.self.value.bias\n",
      "bert.encoder.layer.15.attention.output.dense.weight\n",
      "bert.encoder.layer.15.attention.output.dense.bias\n",
      "bert.encoder.layer.15.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.15.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.15.intermediate.dense.weight\n",
      "bert.encoder.layer.15.intermediate.dense.bias\n",
      "bert.encoder.layer.15.output.dense.weight\n",
      "bert.encoder.layer.15.output.dense.bias\n",
      "bert.encoder.layer.15.output.LayerNorm.weight\n",
      "bert.encoder.layer.15.output.LayerNorm.bias\n",
      "bert.encoder.layer.16.attention.self.query.weight\n",
      "bert.encoder.layer.16.attention.self.query.bias\n",
      "bert.encoder.layer.16.attention.self.key.weight\n",
      "bert.encoder.layer.16.attention.self.key.bias\n",
      "bert.encoder.layer.16.attention.self.value.weight\n",
      "bert.encoder.layer.16.attention.self.value.bias\n",
      "bert.encoder.layer.16.attention.output.dense.weight\n",
      "bert.encoder.layer.16.attention.output.dense.bias\n",
      "bert.encoder.layer.16.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.16.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.16.intermediate.dense.weight\n",
      "bert.encoder.layer.16.intermediate.dense.bias\n",
      "bert.encoder.layer.16.output.dense.weight\n",
      "bert.encoder.layer.16.output.dense.bias\n",
      "bert.encoder.layer.16.output.LayerNorm.weight\n",
      "bert.encoder.layer.16.output.LayerNorm.bias\n",
      "bert.encoder.layer.17.attention.self.query.weight\n",
      "bert.encoder.layer.17.attention.self.query.bias\n",
      "bert.encoder.layer.17.attention.self.key.weight\n",
      "bert.encoder.layer.17.attention.self.key.bias\n",
      "bert.encoder.layer.17.attention.self.value.weight\n",
      "bert.encoder.layer.17.attention.self.value.bias\n",
      "bert.encoder.layer.17.attention.output.dense.weight\n",
      "bert.encoder.layer.17.attention.output.dense.bias\n",
      "bert.encoder.layer.17.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.17.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.17.intermediate.dense.weight\n",
      "bert.encoder.layer.17.intermediate.dense.bias\n",
      "bert.encoder.layer.17.output.dense.weight\n",
      "bert.encoder.layer.17.output.dense.bias\n",
      "bert.encoder.layer.17.output.LayerNorm.weight\n",
      "bert.encoder.layer.17.output.LayerNorm.bias\n",
      "bert.encoder.layer.18.attention.self.query.weight\n",
      "bert.encoder.layer.18.attention.self.query.bias\n",
      "bert.encoder.layer.18.attention.self.key.weight\n",
      "bert.encoder.layer.18.attention.self.key.bias\n",
      "bert.encoder.layer.18.attention.self.value.weight\n",
      "bert.encoder.layer.18.attention.self.value.bias\n",
      "bert.encoder.layer.18.attention.output.dense.weight\n",
      "bert.encoder.layer.18.attention.output.dense.bias\n",
      "bert.encoder.layer.18.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.18.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.18.intermediate.dense.weight\n",
      "bert.encoder.layer.18.intermediate.dense.bias\n",
      "bert.encoder.layer.18.output.dense.weight\n",
      "bert.encoder.layer.18.output.dense.bias\n",
      "bert.encoder.layer.18.output.LayerNorm.weight\n",
      "bert.encoder.layer.18.output.LayerNorm.bias\n",
      "bert.encoder.layer.19.attention.self.query.weight\n",
      "bert.encoder.layer.19.attention.self.query.bias\n",
      "bert.encoder.layer.19.attention.self.key.weight\n",
      "bert.encoder.layer.19.attention.self.key.bias\n",
      "bert.encoder.layer.19.attention.self.value.weight\n",
      "bert.encoder.layer.19.attention.self.value.bias\n",
      "bert.encoder.layer.19.attention.output.dense.weight\n",
      "bert.encoder.layer.19.attention.output.dense.bias\n",
      "bert.encoder.layer.19.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.19.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.19.intermediate.dense.weight\n",
      "bert.encoder.layer.19.intermediate.dense.bias\n",
      "bert.encoder.layer.19.output.dense.weight\n",
      "bert.encoder.layer.19.output.dense.bias\n",
      "bert.encoder.layer.19.output.LayerNorm.weight\n",
      "bert.encoder.layer.19.output.LayerNorm.bias\n",
      "bert.encoder.layer.20.attention.self.query.weight\n",
      "bert.encoder.layer.20.attention.self.query.bias\n",
      "bert.encoder.layer.20.attention.self.key.weight\n",
      "bert.encoder.layer.20.attention.self.key.bias\n",
      "bert.encoder.layer.20.attention.self.value.weight\n",
      "bert.encoder.layer.20.attention.self.value.bias\n",
      "bert.encoder.layer.20.attention.output.dense.weight\n",
      "bert.encoder.layer.20.attention.output.dense.bias\n",
      "bert.encoder.layer.20.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.20.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.20.intermediate.dense.weight\n",
      "bert.encoder.layer.20.intermediate.dense.bias\n",
      "bert.encoder.layer.20.output.dense.weight\n",
      "bert.encoder.layer.20.output.dense.bias\n",
      "bert.encoder.layer.20.output.LayerNorm.weight\n",
      "bert.encoder.layer.20.output.LayerNorm.bias\n",
      "bert.encoder.layer.21.attention.self.query.weight\n",
      "bert.encoder.layer.21.attention.self.query.bias\n",
      "bert.encoder.layer.21.attention.self.key.weight\n",
      "bert.encoder.layer.21.attention.self.key.bias\n",
      "bert.encoder.layer.21.attention.self.value.weight\n",
      "bert.encoder.layer.21.attention.self.value.bias\n",
      "bert.encoder.layer.21.attention.output.dense.weight\n",
      "bert.encoder.layer.21.attention.output.dense.bias\n",
      "bert.encoder.layer.21.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.21.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.21.intermediate.dense.weight\n",
      "bert.encoder.layer.21.intermediate.dense.bias\n",
      "bert.encoder.layer.21.output.dense.weight\n",
      "bert.encoder.layer.21.output.dense.bias\n",
      "bert.encoder.layer.21.output.LayerNorm.weight\n",
      "bert.encoder.layer.21.output.LayerNorm.bias\n",
      "bert.encoder.layer.22.attention.self.query.weight\n",
      "bert.encoder.layer.22.attention.self.query.bias\n",
      "bert.encoder.layer.22.attention.self.key.weight\n",
      "bert.encoder.layer.22.attention.self.key.bias\n",
      "bert.encoder.layer.22.attention.self.value.weight\n",
      "bert.encoder.layer.22.attention.self.value.bias\n",
      "bert.encoder.layer.22.attention.output.dense.weight\n",
      "bert.encoder.layer.22.attention.output.dense.bias\n",
      "bert.encoder.layer.22.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.22.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.22.intermediate.dense.weight\n",
      "bert.encoder.layer.22.intermediate.dense.bias\n",
      "bert.encoder.layer.22.output.dense.weight\n",
      "bert.encoder.layer.22.output.dense.bias\n",
      "bert.encoder.layer.22.output.LayerNorm.weight\n",
      "bert.encoder.layer.22.output.LayerNorm.bias\n",
      "bert.encoder.layer.23.attention.self.query.weight\n",
      "bert.encoder.layer.23.attention.self.query.bias\n",
      "bert.encoder.layer.23.attention.self.key.weight\n",
      "bert.encoder.layer.23.attention.self.key.bias\n",
      "bert.encoder.layer.23.attention.self.value.weight\n",
      "bert.encoder.layer.23.attention.self.value.bias\n",
      "bert.encoder.layer.23.attention.output.dense.weight\n",
      "bert.encoder.layer.23.attention.output.dense.bias\n",
      "bert.encoder.layer.23.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.23.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.23.intermediate.dense.weight\n",
      "bert.encoder.layer.23.intermediate.dense.bias\n",
      "bert.encoder.layer.23.output.dense.weight\n",
      "bert.encoder.layer.23.output.dense.bias\n",
      "bert.encoder.layer.23.output.LayerNorm.weight\n",
      "bert.encoder.layer.23.output.LayerNorm.bias\n",
      "bert.pooler.dense.weight\n",
      "bert.pooler.dense.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():                \n",
    "    if param.requires_grad:\n",
    "        if name.startswith(\"bert\"):\n",
    "            print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1980f31-187b-41ba-92c2-9e3dcee792cf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag_layer1.self_attn.in_proj_weight\n",
      "tag_layer1.self_attn.in_proj_bias\n",
      "tag_layer1.self_attn.out_proj.weight\n",
      "tag_layer1.self_attn.out_proj.bias\n",
      "tag_layer1.linear1.weight\n",
      "tag_layer1.linear1.bias\n",
      "tag_layer1.linear2.weight\n",
      "tag_layer1.linear2.bias\n",
      "tag_layer1.norm1.weight\n",
      "tag_layer1.norm1.bias\n",
      "tag_layer1.norm2.weight\n",
      "tag_layer1.norm2.bias\n",
      "cls_layer1.self_attn.in_proj_weight\n",
      "cls_layer1.self_attn.in_proj_bias\n",
      "cls_layer1.self_attn.out_proj.weight\n",
      "cls_layer1.self_attn.out_proj.bias\n",
      "cls_layer1.linear1.weight\n",
      "cls_layer1.linear1.bias\n",
      "cls_layer1.linear2.weight\n",
      "cls_layer1.linear2.bias\n",
      "cls_layer1.norm1.weight\n",
      "cls_layer1.norm1.bias\n",
      "cls_layer1.norm2.weight\n",
      "cls_layer1.norm2.bias\n",
      "tag_layer2.self_attn.in_proj_weight\n",
      "tag_layer2.self_attn.in_proj_bias\n",
      "tag_layer2.self_attn.out_proj.weight\n",
      "tag_layer2.self_attn.out_proj.bias\n",
      "tag_layer2.linear1.weight\n",
      "tag_layer2.linear1.bias\n",
      "tag_layer2.linear2.weight\n",
      "tag_layer2.linear2.bias\n",
      "tag_layer2.norm1.weight\n",
      "tag_layer2.norm1.bias\n",
      "tag_layer2.norm2.weight\n",
      "tag_layer2.norm2.bias\n",
      "cls_layer2.self_attn.in_proj_weight\n",
      "cls_layer2.self_attn.in_proj_bias\n",
      "cls_layer2.self_attn.out_proj.weight\n",
      "cls_layer2.self_attn.out_proj.bias\n",
      "cls_layer2.linear1.weight\n",
      "cls_layer2.linear1.bias\n",
      "cls_layer2.linear2.weight\n",
      "cls_layer2.linear2.bias\n",
      "cls_layer2.norm1.weight\n",
      "cls_layer2.norm1.bias\n",
      "cls_layer2.norm2.weight\n",
      "cls_layer2.norm2.bias\n",
      "fc_tag.weight\n",
      "fc_tag.bias\n",
      "fc_cls.weight\n",
      "fc_cls.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():                \n",
    "    if param.requires_grad:\n",
    "        if not name.startswith(\"bert\"):\n",
    "            print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a75a5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.optimization import (\n",
    "    get_linear_schedule_with_warmup, get_constant_schedule, get_constant_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    "get_cosine_schedule_with_warmup)\n",
    "\n",
    "TAG_PAD_IDX = LABEL1.vocab.stoi[LABEL1.pad_token]\n",
    "\n",
    "criterion_tag = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
    "criterion_cls = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
    "\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "criterion_tag = criterion_tag.to(device)\n",
    "criterion_cls = criterion_cls.to(device)\n",
    "\n",
    "#optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    [\n",
    "        {\"params\": bert_params, \"lr\": 0.9*LEARNING_RATE},\n",
    "        {\"params\": other_params},\n",
    "    ],\n",
    "    lr=LEARNING_RATE,\n",
    ")\n",
    "\n",
    "# optimizer = optim.Adam(\n",
    "#     [\n",
    "#         {\"params\": bert_params, \"lr\": 2*LEARNING_RATE}\n",
    "#         {\"params\": other_params}\n",
    "#     ],\n",
    "#     lr=LEARNING_RATE,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f916de3-69e4-499f-af5d-32cb19956d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.bert.named_parameters():                \n",
    "#     if name.startswith('embeddings'):\n",
    "#         param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdfb3735-b997-4630-a493-8880fa71ea15",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight\n",
      "bert.embeddings.position_embeddings.weight\n",
      "bert.embeddings.token_type_embeddings.weight\n",
      "bert.embeddings.LayerNorm.weight\n",
      "bert.embeddings.LayerNorm.bias\n",
      "bert.encoder.layer.0.attention.self.query.weight\n",
      "bert.encoder.layer.0.attention.self.query.bias\n",
      "bert.encoder.layer.0.attention.self.key.weight\n",
      "bert.encoder.layer.0.attention.self.key.bias\n",
      "bert.encoder.layer.0.attention.self.value.weight\n",
      "bert.encoder.layer.0.attention.self.value.bias\n",
      "bert.encoder.layer.0.attention.output.dense.weight\n",
      "bert.encoder.layer.0.attention.output.dense.bias\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.0.intermediate.dense.weight\n",
      "bert.encoder.layer.0.intermediate.dense.bias\n",
      "bert.encoder.layer.0.output.dense.weight\n",
      "bert.encoder.layer.0.output.dense.bias\n",
      "bert.encoder.layer.0.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.attention.self.query.weight\n",
      "bert.encoder.layer.1.attention.self.query.bias\n",
      "bert.encoder.layer.1.attention.self.key.weight\n",
      "bert.encoder.layer.1.attention.self.key.bias\n",
      "bert.encoder.layer.1.attention.self.value.weight\n",
      "bert.encoder.layer.1.attention.self.value.bias\n",
      "bert.encoder.layer.1.attention.output.dense.weight\n",
      "bert.encoder.layer.1.attention.output.dense.bias\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.intermediate.dense.weight\n",
      "bert.encoder.layer.1.intermediate.dense.bias\n",
      "bert.encoder.layer.1.output.dense.weight\n",
      "bert.encoder.layer.1.output.dense.bias\n",
      "bert.encoder.layer.1.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.attention.self.query.weight\n",
      "bert.encoder.layer.2.attention.self.query.bias\n",
      "bert.encoder.layer.2.attention.self.key.weight\n",
      "bert.encoder.layer.2.attention.self.key.bias\n",
      "bert.encoder.layer.2.attention.self.value.weight\n",
      "bert.encoder.layer.2.attention.self.value.bias\n",
      "bert.encoder.layer.2.attention.output.dense.weight\n",
      "bert.encoder.layer.2.attention.output.dense.bias\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.intermediate.dense.weight\n",
      "bert.encoder.layer.2.intermediate.dense.bias\n",
      "bert.encoder.layer.2.output.dense.weight\n",
      "bert.encoder.layer.2.output.dense.bias\n",
      "bert.encoder.layer.2.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.attention.self.query.weight\n",
      "bert.encoder.layer.3.attention.self.query.bias\n",
      "bert.encoder.layer.3.attention.self.key.weight\n",
      "bert.encoder.layer.3.attention.self.key.bias\n",
      "bert.encoder.layer.3.attention.self.value.weight\n",
      "bert.encoder.layer.3.attention.self.value.bias\n",
      "bert.encoder.layer.3.attention.output.dense.weight\n",
      "bert.encoder.layer.3.attention.output.dense.bias\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.intermediate.dense.weight\n",
      "bert.encoder.layer.3.intermediate.dense.bias\n",
      "bert.encoder.layer.3.output.dense.weight\n",
      "bert.encoder.layer.3.output.dense.bias\n",
      "bert.encoder.layer.3.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.attention.self.query.weight\n",
      "bert.encoder.layer.4.attention.self.query.bias\n",
      "bert.encoder.layer.4.attention.self.key.weight\n",
      "bert.encoder.layer.4.attention.self.key.bias\n",
      "bert.encoder.layer.4.attention.self.value.weight\n",
      "bert.encoder.layer.4.attention.self.value.bias\n",
      "bert.encoder.layer.4.attention.output.dense.weight\n",
      "bert.encoder.layer.4.attention.output.dense.bias\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.intermediate.dense.weight\n",
      "bert.encoder.layer.4.intermediate.dense.bias\n",
      "bert.encoder.layer.4.output.dense.weight\n",
      "bert.encoder.layer.4.output.dense.bias\n",
      "bert.encoder.layer.4.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.attention.self.query.weight\n",
      "bert.encoder.layer.5.attention.self.query.bias\n",
      "bert.encoder.layer.5.attention.self.key.weight\n",
      "bert.encoder.layer.5.attention.self.key.bias\n",
      "bert.encoder.layer.5.attention.self.value.weight\n",
      "bert.encoder.layer.5.attention.self.value.bias\n",
      "bert.encoder.layer.5.attention.output.dense.weight\n",
      "bert.encoder.layer.5.attention.output.dense.bias\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.intermediate.dense.weight\n",
      "bert.encoder.layer.5.intermediate.dense.bias\n",
      "bert.encoder.layer.5.output.dense.weight\n",
      "bert.encoder.layer.5.output.dense.bias\n",
      "bert.encoder.layer.5.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.attention.self.query.weight\n",
      "bert.encoder.layer.6.attention.self.query.bias\n",
      "bert.encoder.layer.6.attention.self.key.weight\n",
      "bert.encoder.layer.6.attention.self.key.bias\n",
      "bert.encoder.layer.6.attention.self.value.weight\n",
      "bert.encoder.layer.6.attention.self.value.bias\n",
      "bert.encoder.layer.6.attention.output.dense.weight\n",
      "bert.encoder.layer.6.attention.output.dense.bias\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.intermediate.dense.weight\n",
      "bert.encoder.layer.6.intermediate.dense.bias\n",
      "bert.encoder.layer.6.output.dense.weight\n",
      "bert.encoder.layer.6.output.dense.bias\n",
      "bert.encoder.layer.6.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.attention.self.query.weight\n",
      "bert.encoder.layer.7.attention.self.query.bias\n",
      "bert.encoder.layer.7.attention.self.key.weight\n",
      "bert.encoder.layer.7.attention.self.key.bias\n",
      "bert.encoder.layer.7.attention.self.value.weight\n",
      "bert.encoder.layer.7.attention.self.value.bias\n",
      "bert.encoder.layer.7.attention.output.dense.weight\n",
      "bert.encoder.layer.7.attention.output.dense.bias\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.intermediate.dense.weight\n",
      "bert.encoder.layer.7.intermediate.dense.bias\n",
      "bert.encoder.layer.7.output.dense.weight\n",
      "bert.encoder.layer.7.output.dense.bias\n",
      "bert.encoder.layer.7.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.attention.self.query.weight\n",
      "bert.encoder.layer.8.attention.self.query.bias\n",
      "bert.encoder.layer.8.attention.self.key.weight\n",
      "bert.encoder.layer.8.attention.self.key.bias\n",
      "bert.encoder.layer.8.attention.self.value.weight\n",
      "bert.encoder.layer.8.attention.self.value.bias\n",
      "bert.encoder.layer.8.attention.output.dense.weight\n",
      "bert.encoder.layer.8.attention.output.dense.bias\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.intermediate.dense.weight\n",
      "bert.encoder.layer.8.intermediate.dense.bias\n",
      "bert.encoder.layer.8.output.dense.weight\n",
      "bert.encoder.layer.8.output.dense.bias\n",
      "bert.encoder.layer.8.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.attention.self.query.weight\n",
      "bert.encoder.layer.9.attention.self.query.bias\n",
      "bert.encoder.layer.9.attention.self.key.weight\n",
      "bert.encoder.layer.9.attention.self.key.bias\n",
      "bert.encoder.layer.9.attention.self.value.weight\n",
      "bert.encoder.layer.9.attention.self.value.bias\n",
      "bert.encoder.layer.9.attention.output.dense.weight\n",
      "bert.encoder.layer.9.attention.output.dense.bias\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.intermediate.dense.weight\n",
      "bert.encoder.layer.9.intermediate.dense.bias\n",
      "bert.encoder.layer.9.output.dense.weight\n",
      "bert.encoder.layer.9.output.dense.bias\n",
      "bert.encoder.layer.9.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.attention.self.query.weight\n",
      "bert.encoder.layer.10.attention.self.query.bias\n",
      "bert.encoder.layer.10.attention.self.key.weight\n",
      "bert.encoder.layer.10.attention.self.key.bias\n",
      "bert.encoder.layer.10.attention.self.value.weight\n",
      "bert.encoder.layer.10.attention.self.value.bias\n",
      "bert.encoder.layer.10.attention.output.dense.weight\n",
      "bert.encoder.layer.10.attention.output.dense.bias\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.intermediate.dense.weight\n",
      "bert.encoder.layer.10.intermediate.dense.bias\n",
      "bert.encoder.layer.10.output.dense.weight\n",
      "bert.encoder.layer.10.output.dense.bias\n",
      "bert.encoder.layer.10.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.attention.self.query.weight\n",
      "bert.encoder.layer.11.attention.self.query.bias\n",
      "bert.encoder.layer.11.attention.self.key.weight\n",
      "bert.encoder.layer.11.attention.self.key.bias\n",
      "bert.encoder.layer.11.attention.self.value.weight\n",
      "bert.encoder.layer.11.attention.self.value.bias\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "bert.encoder.layer.11.attention.output.dense.bias\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.intermediate.dense.weight\n",
      "bert.encoder.layer.11.intermediate.dense.bias\n",
      "bert.encoder.layer.11.output.dense.weight\n",
      "bert.encoder.layer.11.output.dense.bias\n",
      "bert.encoder.layer.11.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.output.LayerNorm.bias\n",
      "bert.encoder.layer.12.attention.self.query.weight\n",
      "bert.encoder.layer.12.attention.self.query.bias\n",
      "bert.encoder.layer.12.attention.self.key.weight\n",
      "bert.encoder.layer.12.attention.self.key.bias\n",
      "bert.encoder.layer.12.attention.self.value.weight\n",
      "bert.encoder.layer.12.attention.self.value.bias\n",
      "bert.encoder.layer.12.attention.output.dense.weight\n",
      "bert.encoder.layer.12.attention.output.dense.bias\n",
      "bert.encoder.layer.12.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.12.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.12.intermediate.dense.weight\n",
      "bert.encoder.layer.12.intermediate.dense.bias\n",
      "bert.encoder.layer.12.output.dense.weight\n",
      "bert.encoder.layer.12.output.dense.bias\n",
      "bert.encoder.layer.12.output.LayerNorm.weight\n",
      "bert.encoder.layer.12.output.LayerNorm.bias\n",
      "bert.encoder.layer.13.attention.self.query.weight\n",
      "bert.encoder.layer.13.attention.self.query.bias\n",
      "bert.encoder.layer.13.attention.self.key.weight\n",
      "bert.encoder.layer.13.attention.self.key.bias\n",
      "bert.encoder.layer.13.attention.self.value.weight\n",
      "bert.encoder.layer.13.attention.self.value.bias\n",
      "bert.encoder.layer.13.attention.output.dense.weight\n",
      "bert.encoder.layer.13.attention.output.dense.bias\n",
      "bert.encoder.layer.13.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.13.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.13.intermediate.dense.weight\n",
      "bert.encoder.layer.13.intermediate.dense.bias\n",
      "bert.encoder.layer.13.output.dense.weight\n",
      "bert.encoder.layer.13.output.dense.bias\n",
      "bert.encoder.layer.13.output.LayerNorm.weight\n",
      "bert.encoder.layer.13.output.LayerNorm.bias\n",
      "bert.encoder.layer.14.attention.self.query.weight\n",
      "bert.encoder.layer.14.attention.self.query.bias\n",
      "bert.encoder.layer.14.attention.self.key.weight\n",
      "bert.encoder.layer.14.attention.self.key.bias\n",
      "bert.encoder.layer.14.attention.self.value.weight\n",
      "bert.encoder.layer.14.attention.self.value.bias\n",
      "bert.encoder.layer.14.attention.output.dense.weight\n",
      "bert.encoder.layer.14.attention.output.dense.bias\n",
      "bert.encoder.layer.14.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.14.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.14.intermediate.dense.weight\n",
      "bert.encoder.layer.14.intermediate.dense.bias\n",
      "bert.encoder.layer.14.output.dense.weight\n",
      "bert.encoder.layer.14.output.dense.bias\n",
      "bert.encoder.layer.14.output.LayerNorm.weight\n",
      "bert.encoder.layer.14.output.LayerNorm.bias\n",
      "bert.encoder.layer.15.attention.self.query.weight\n",
      "bert.encoder.layer.15.attention.self.query.bias\n",
      "bert.encoder.layer.15.attention.self.key.weight\n",
      "bert.encoder.layer.15.attention.self.key.bias\n",
      "bert.encoder.layer.15.attention.self.value.weight\n",
      "bert.encoder.layer.15.attention.self.value.bias\n",
      "bert.encoder.layer.15.attention.output.dense.weight\n",
      "bert.encoder.layer.15.attention.output.dense.bias\n",
      "bert.encoder.layer.15.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.15.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.15.intermediate.dense.weight\n",
      "bert.encoder.layer.15.intermediate.dense.bias\n",
      "bert.encoder.layer.15.output.dense.weight\n",
      "bert.encoder.layer.15.output.dense.bias\n",
      "bert.encoder.layer.15.output.LayerNorm.weight\n",
      "bert.encoder.layer.15.output.LayerNorm.bias\n",
      "bert.encoder.layer.16.attention.self.query.weight\n",
      "bert.encoder.layer.16.attention.self.query.bias\n",
      "bert.encoder.layer.16.attention.self.key.weight\n",
      "bert.encoder.layer.16.attention.self.key.bias\n",
      "bert.encoder.layer.16.attention.self.value.weight\n",
      "bert.encoder.layer.16.attention.self.value.bias\n",
      "bert.encoder.layer.16.attention.output.dense.weight\n",
      "bert.encoder.layer.16.attention.output.dense.bias\n",
      "bert.encoder.layer.16.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.16.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.16.intermediate.dense.weight\n",
      "bert.encoder.layer.16.intermediate.dense.bias\n",
      "bert.encoder.layer.16.output.dense.weight\n",
      "bert.encoder.layer.16.output.dense.bias\n",
      "bert.encoder.layer.16.output.LayerNorm.weight\n",
      "bert.encoder.layer.16.output.LayerNorm.bias\n",
      "bert.encoder.layer.17.attention.self.query.weight\n",
      "bert.encoder.layer.17.attention.self.query.bias\n",
      "bert.encoder.layer.17.attention.self.key.weight\n",
      "bert.encoder.layer.17.attention.self.key.bias\n",
      "bert.encoder.layer.17.attention.self.value.weight\n",
      "bert.encoder.layer.17.attention.self.value.bias\n",
      "bert.encoder.layer.17.attention.output.dense.weight\n",
      "bert.encoder.layer.17.attention.output.dense.bias\n",
      "bert.encoder.layer.17.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.17.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.17.intermediate.dense.weight\n",
      "bert.encoder.layer.17.intermediate.dense.bias\n",
      "bert.encoder.layer.17.output.dense.weight\n",
      "bert.encoder.layer.17.output.dense.bias\n",
      "bert.encoder.layer.17.output.LayerNorm.weight\n",
      "bert.encoder.layer.17.output.LayerNorm.bias\n",
      "bert.encoder.layer.18.attention.self.query.weight\n",
      "bert.encoder.layer.18.attention.self.query.bias\n",
      "bert.encoder.layer.18.attention.self.key.weight\n",
      "bert.encoder.layer.18.attention.self.key.bias\n",
      "bert.encoder.layer.18.attention.self.value.weight\n",
      "bert.encoder.layer.18.attention.self.value.bias\n",
      "bert.encoder.layer.18.attention.output.dense.weight\n",
      "bert.encoder.layer.18.attention.output.dense.bias\n",
      "bert.encoder.layer.18.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.18.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.18.intermediate.dense.weight\n",
      "bert.encoder.layer.18.intermediate.dense.bias\n",
      "bert.encoder.layer.18.output.dense.weight\n",
      "bert.encoder.layer.18.output.dense.bias\n",
      "bert.encoder.layer.18.output.LayerNorm.weight\n",
      "bert.encoder.layer.18.output.LayerNorm.bias\n",
      "bert.encoder.layer.19.attention.self.query.weight\n",
      "bert.encoder.layer.19.attention.self.query.bias\n",
      "bert.encoder.layer.19.attention.self.key.weight\n",
      "bert.encoder.layer.19.attention.self.key.bias\n",
      "bert.encoder.layer.19.attention.self.value.weight\n",
      "bert.encoder.layer.19.attention.self.value.bias\n",
      "bert.encoder.layer.19.attention.output.dense.weight\n",
      "bert.encoder.layer.19.attention.output.dense.bias\n",
      "bert.encoder.layer.19.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.19.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.19.intermediate.dense.weight\n",
      "bert.encoder.layer.19.intermediate.dense.bias\n",
      "bert.encoder.layer.19.output.dense.weight\n",
      "bert.encoder.layer.19.output.dense.bias\n",
      "bert.encoder.layer.19.output.LayerNorm.weight\n",
      "bert.encoder.layer.19.output.LayerNorm.bias\n",
      "bert.encoder.layer.20.attention.self.query.weight\n",
      "bert.encoder.layer.20.attention.self.query.bias\n",
      "bert.encoder.layer.20.attention.self.key.weight\n",
      "bert.encoder.layer.20.attention.self.key.bias\n",
      "bert.encoder.layer.20.attention.self.value.weight\n",
      "bert.encoder.layer.20.attention.self.value.bias\n",
      "bert.encoder.layer.20.attention.output.dense.weight\n",
      "bert.encoder.layer.20.attention.output.dense.bias\n",
      "bert.encoder.layer.20.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.20.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.20.intermediate.dense.weight\n",
      "bert.encoder.layer.20.intermediate.dense.bias\n",
      "bert.encoder.layer.20.output.dense.weight\n",
      "bert.encoder.layer.20.output.dense.bias\n",
      "bert.encoder.layer.20.output.LayerNorm.weight\n",
      "bert.encoder.layer.20.output.LayerNorm.bias\n",
      "bert.encoder.layer.21.attention.self.query.weight\n",
      "bert.encoder.layer.21.attention.self.query.bias\n",
      "bert.encoder.layer.21.attention.self.key.weight\n",
      "bert.encoder.layer.21.attention.self.key.bias\n",
      "bert.encoder.layer.21.attention.self.value.weight\n",
      "bert.encoder.layer.21.attention.self.value.bias\n",
      "bert.encoder.layer.21.attention.output.dense.weight\n",
      "bert.encoder.layer.21.attention.output.dense.bias\n",
      "bert.encoder.layer.21.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.21.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.21.intermediate.dense.weight\n",
      "bert.encoder.layer.21.intermediate.dense.bias\n",
      "bert.encoder.layer.21.output.dense.weight\n",
      "bert.encoder.layer.21.output.dense.bias\n",
      "bert.encoder.layer.21.output.LayerNorm.weight\n",
      "bert.encoder.layer.21.output.LayerNorm.bias\n",
      "bert.encoder.layer.22.attention.self.query.weight\n",
      "bert.encoder.layer.22.attention.self.query.bias\n",
      "bert.encoder.layer.22.attention.self.key.weight\n",
      "bert.encoder.layer.22.attention.self.key.bias\n",
      "bert.encoder.layer.22.attention.self.value.weight\n",
      "bert.encoder.layer.22.attention.self.value.bias\n",
      "bert.encoder.layer.22.attention.output.dense.weight\n",
      "bert.encoder.layer.22.attention.output.dense.bias\n",
      "bert.encoder.layer.22.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.22.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.22.intermediate.dense.weight\n",
      "bert.encoder.layer.22.intermediate.dense.bias\n",
      "bert.encoder.layer.22.output.dense.weight\n",
      "bert.encoder.layer.22.output.dense.bias\n",
      "bert.encoder.layer.22.output.LayerNorm.weight\n",
      "bert.encoder.layer.22.output.LayerNorm.bias\n",
      "bert.encoder.layer.23.attention.self.query.weight\n",
      "bert.encoder.layer.23.attention.self.query.bias\n",
      "bert.encoder.layer.23.attention.self.key.weight\n",
      "bert.encoder.layer.23.attention.self.key.bias\n",
      "bert.encoder.layer.23.attention.self.value.weight\n",
      "bert.encoder.layer.23.attention.self.value.bias\n",
      "bert.encoder.layer.23.attention.output.dense.weight\n",
      "bert.encoder.layer.23.attention.output.dense.bias\n",
      "bert.encoder.layer.23.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.23.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.23.intermediate.dense.weight\n",
      "bert.encoder.layer.23.intermediate.dense.bias\n",
      "bert.encoder.layer.23.output.dense.weight\n",
      "bert.encoder.layer.23.output.dense.bias\n",
      "bert.encoder.layer.23.output.LayerNorm.weight\n",
      "bert.encoder.layer.23.output.LayerNorm.bias\n",
      "bert.pooler.dense.weight\n",
      "bert.pooler.dense.bias\n",
      "tag_layer1.self_attn.in_proj_weight\n",
      "tag_layer1.self_attn.in_proj_bias\n",
      "tag_layer1.self_attn.out_proj.weight\n",
      "tag_layer1.self_attn.out_proj.bias\n",
      "tag_layer1.linear1.weight\n",
      "tag_layer1.linear1.bias\n",
      "tag_layer1.linear2.weight\n",
      "tag_layer1.linear2.bias\n",
      "tag_layer1.norm1.weight\n",
      "tag_layer1.norm1.bias\n",
      "tag_layer1.norm2.weight\n",
      "tag_layer1.norm2.bias\n",
      "cls_layer1.self_attn.in_proj_weight\n",
      "cls_layer1.self_attn.in_proj_bias\n",
      "cls_layer1.self_attn.out_proj.weight\n",
      "cls_layer1.self_attn.out_proj.bias\n",
      "cls_layer1.linear1.weight\n",
      "cls_layer1.linear1.bias\n",
      "cls_layer1.linear2.weight\n",
      "cls_layer1.linear2.bias\n",
      "cls_layer1.norm1.weight\n",
      "cls_layer1.norm1.bias\n",
      "cls_layer1.norm2.weight\n",
      "cls_layer1.norm2.bias\n",
      "tag_layer2.self_attn.in_proj_weight\n",
      "tag_layer2.self_attn.in_proj_bias\n",
      "tag_layer2.self_attn.out_proj.weight\n",
      "tag_layer2.self_attn.out_proj.bias\n",
      "tag_layer2.linear1.weight\n",
      "tag_layer2.linear1.bias\n",
      "tag_layer2.linear2.weight\n",
      "tag_layer2.linear2.bias\n",
      "tag_layer2.norm1.weight\n",
      "tag_layer2.norm1.bias\n",
      "tag_layer2.norm2.weight\n",
      "tag_layer2.norm2.bias\n",
      "cls_layer2.self_attn.in_proj_weight\n",
      "cls_layer2.self_attn.in_proj_bias\n",
      "cls_layer2.self_attn.out_proj.weight\n",
      "cls_layer2.self_attn.out_proj.bias\n",
      "cls_layer2.linear1.weight\n",
      "cls_layer2.linear1.bias\n",
      "cls_layer2.linear2.weight\n",
      "cls_layer2.linear2.bias\n",
      "cls_layer2.norm1.weight\n",
      "cls_layer2.norm1.bias\n",
      "cls_layer2.norm2.weight\n",
      "cls_layer2.norm2.bias\n",
      "fc_tag.weight\n",
      "fc_tag.bias\n",
      "fc_cls.weight\n",
      "fc_cls.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():                \n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a491051-4c10-4b28-8878-e78baa48e52f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight\n",
      "embeddings.position_embeddings.weight\n",
      "embeddings.token_type_embeddings.weight\n",
      "embeddings.LayerNorm.weight\n",
      "embeddings.LayerNorm.bias\n",
      "encoder.layer.0.attention.self.query.weight\n",
      "encoder.layer.0.attention.self.query.bias\n",
      "encoder.layer.0.attention.self.key.weight\n",
      "encoder.layer.0.attention.self.key.bias\n",
      "encoder.layer.0.attention.self.value.weight\n",
      "encoder.layer.0.attention.self.value.bias\n",
      "encoder.layer.0.attention.output.dense.weight\n",
      "encoder.layer.0.attention.output.dense.bias\n",
      "encoder.layer.0.attention.output.LayerNorm.weight\n",
      "encoder.layer.0.attention.output.LayerNorm.bias\n",
      "encoder.layer.0.intermediate.dense.weight\n",
      "encoder.layer.0.intermediate.dense.bias\n",
      "encoder.layer.0.output.dense.weight\n",
      "encoder.layer.0.output.dense.bias\n",
      "encoder.layer.0.output.LayerNorm.weight\n",
      "encoder.layer.0.output.LayerNorm.bias\n",
      "encoder.layer.1.attention.self.query.weight\n",
      "encoder.layer.1.attention.self.query.bias\n",
      "encoder.layer.1.attention.self.key.weight\n",
      "encoder.layer.1.attention.self.key.bias\n",
      "encoder.layer.1.attention.self.value.weight\n",
      "encoder.layer.1.attention.self.value.bias\n",
      "encoder.layer.1.attention.output.dense.weight\n",
      "encoder.layer.1.attention.output.dense.bias\n",
      "encoder.layer.1.attention.output.LayerNorm.weight\n",
      "encoder.layer.1.attention.output.LayerNorm.bias\n",
      "encoder.layer.1.intermediate.dense.weight\n",
      "encoder.layer.1.intermediate.dense.bias\n",
      "encoder.layer.1.output.dense.weight\n",
      "encoder.layer.1.output.dense.bias\n",
      "encoder.layer.1.output.LayerNorm.weight\n",
      "encoder.layer.1.output.LayerNorm.bias\n",
      "encoder.layer.2.attention.self.query.weight\n",
      "encoder.layer.2.attention.self.query.bias\n",
      "encoder.layer.2.attention.self.key.weight\n",
      "encoder.layer.2.attention.self.key.bias\n",
      "encoder.layer.2.attention.self.value.weight\n",
      "encoder.layer.2.attention.self.value.bias\n",
      "encoder.layer.2.attention.output.dense.weight\n",
      "encoder.layer.2.attention.output.dense.bias\n",
      "encoder.layer.2.attention.output.LayerNorm.weight\n",
      "encoder.layer.2.attention.output.LayerNorm.bias\n",
      "encoder.layer.2.intermediate.dense.weight\n",
      "encoder.layer.2.intermediate.dense.bias\n",
      "encoder.layer.2.output.dense.weight\n",
      "encoder.layer.2.output.dense.bias\n",
      "encoder.layer.2.output.LayerNorm.weight\n",
      "encoder.layer.2.output.LayerNorm.bias\n",
      "encoder.layer.3.attention.self.query.weight\n",
      "encoder.layer.3.attention.self.query.bias\n",
      "encoder.layer.3.attention.self.key.weight\n",
      "encoder.layer.3.attention.self.key.bias\n",
      "encoder.layer.3.attention.self.value.weight\n",
      "encoder.layer.3.attention.self.value.bias\n",
      "encoder.layer.3.attention.output.dense.weight\n",
      "encoder.layer.3.attention.output.dense.bias\n",
      "encoder.layer.3.attention.output.LayerNorm.weight\n",
      "encoder.layer.3.attention.output.LayerNorm.bias\n",
      "encoder.layer.3.intermediate.dense.weight\n",
      "encoder.layer.3.intermediate.dense.bias\n",
      "encoder.layer.3.output.dense.weight\n",
      "encoder.layer.3.output.dense.bias\n",
      "encoder.layer.3.output.LayerNorm.weight\n",
      "encoder.layer.3.output.LayerNorm.bias\n",
      "encoder.layer.4.attention.self.query.weight\n",
      "encoder.layer.4.attention.self.query.bias\n",
      "encoder.layer.4.attention.self.key.weight\n",
      "encoder.layer.4.attention.self.key.bias\n",
      "encoder.layer.4.attention.self.value.weight\n",
      "encoder.layer.4.attention.self.value.bias\n",
      "encoder.layer.4.attention.output.dense.weight\n",
      "encoder.layer.4.attention.output.dense.bias\n",
      "encoder.layer.4.attention.output.LayerNorm.weight\n",
      "encoder.layer.4.attention.output.LayerNorm.bias\n",
      "encoder.layer.4.intermediate.dense.weight\n",
      "encoder.layer.4.intermediate.dense.bias\n",
      "encoder.layer.4.output.dense.weight\n",
      "encoder.layer.4.output.dense.bias\n",
      "encoder.layer.4.output.LayerNorm.weight\n",
      "encoder.layer.4.output.LayerNorm.bias\n",
      "encoder.layer.5.attention.self.query.weight\n",
      "encoder.layer.5.attention.self.query.bias\n",
      "encoder.layer.5.attention.self.key.weight\n",
      "encoder.layer.5.attention.self.key.bias\n",
      "encoder.layer.5.attention.self.value.weight\n",
      "encoder.layer.5.attention.self.value.bias\n",
      "encoder.layer.5.attention.output.dense.weight\n",
      "encoder.layer.5.attention.output.dense.bias\n",
      "encoder.layer.5.attention.output.LayerNorm.weight\n",
      "encoder.layer.5.attention.output.LayerNorm.bias\n",
      "encoder.layer.5.intermediate.dense.weight\n",
      "encoder.layer.5.intermediate.dense.bias\n",
      "encoder.layer.5.output.dense.weight\n",
      "encoder.layer.5.output.dense.bias\n",
      "encoder.layer.5.output.LayerNorm.weight\n",
      "encoder.layer.5.output.LayerNorm.bias\n",
      "encoder.layer.6.attention.self.query.weight\n",
      "encoder.layer.6.attention.self.query.bias\n",
      "encoder.layer.6.attention.self.key.weight\n",
      "encoder.layer.6.attention.self.key.bias\n",
      "encoder.layer.6.attention.self.value.weight\n",
      "encoder.layer.6.attention.self.value.bias\n",
      "encoder.layer.6.attention.output.dense.weight\n",
      "encoder.layer.6.attention.output.dense.bias\n",
      "encoder.layer.6.attention.output.LayerNorm.weight\n",
      "encoder.layer.6.attention.output.LayerNorm.bias\n",
      "encoder.layer.6.intermediate.dense.weight\n",
      "encoder.layer.6.intermediate.dense.bias\n",
      "encoder.layer.6.output.dense.weight\n",
      "encoder.layer.6.output.dense.bias\n",
      "encoder.layer.6.output.LayerNorm.weight\n",
      "encoder.layer.6.output.LayerNorm.bias\n",
      "encoder.layer.7.attention.self.query.weight\n",
      "encoder.layer.7.attention.self.query.bias\n",
      "encoder.layer.7.attention.self.key.weight\n",
      "encoder.layer.7.attention.self.key.bias\n",
      "encoder.layer.7.attention.self.value.weight\n",
      "encoder.layer.7.attention.self.value.bias\n",
      "encoder.layer.7.attention.output.dense.weight\n",
      "encoder.layer.7.attention.output.dense.bias\n",
      "encoder.layer.7.attention.output.LayerNorm.weight\n",
      "encoder.layer.7.attention.output.LayerNorm.bias\n",
      "encoder.layer.7.intermediate.dense.weight\n",
      "encoder.layer.7.intermediate.dense.bias\n",
      "encoder.layer.7.output.dense.weight\n",
      "encoder.layer.7.output.dense.bias\n",
      "encoder.layer.7.output.LayerNorm.weight\n",
      "encoder.layer.7.output.LayerNorm.bias\n",
      "encoder.layer.8.attention.self.query.weight\n",
      "encoder.layer.8.attention.self.query.bias\n",
      "encoder.layer.8.attention.self.key.weight\n",
      "encoder.layer.8.attention.self.key.bias\n",
      "encoder.layer.8.attention.self.value.weight\n",
      "encoder.layer.8.attention.self.value.bias\n",
      "encoder.layer.8.attention.output.dense.weight\n",
      "encoder.layer.8.attention.output.dense.bias\n",
      "encoder.layer.8.attention.output.LayerNorm.weight\n",
      "encoder.layer.8.attention.output.LayerNorm.bias\n",
      "encoder.layer.8.intermediate.dense.weight\n",
      "encoder.layer.8.intermediate.dense.bias\n",
      "encoder.layer.8.output.dense.weight\n",
      "encoder.layer.8.output.dense.bias\n",
      "encoder.layer.8.output.LayerNorm.weight\n",
      "encoder.layer.8.output.LayerNorm.bias\n",
      "encoder.layer.9.attention.self.query.weight\n",
      "encoder.layer.9.attention.self.query.bias\n",
      "encoder.layer.9.attention.self.key.weight\n",
      "encoder.layer.9.attention.self.key.bias\n",
      "encoder.layer.9.attention.self.value.weight\n",
      "encoder.layer.9.attention.self.value.bias\n",
      "encoder.layer.9.attention.output.dense.weight\n",
      "encoder.layer.9.attention.output.dense.bias\n",
      "encoder.layer.9.attention.output.LayerNorm.weight\n",
      "encoder.layer.9.attention.output.LayerNorm.bias\n",
      "encoder.layer.9.intermediate.dense.weight\n",
      "encoder.layer.9.intermediate.dense.bias\n",
      "encoder.layer.9.output.dense.weight\n",
      "encoder.layer.9.output.dense.bias\n",
      "encoder.layer.9.output.LayerNorm.weight\n",
      "encoder.layer.9.output.LayerNorm.bias\n",
      "encoder.layer.10.attention.self.query.weight\n",
      "encoder.layer.10.attention.self.query.bias\n",
      "encoder.layer.10.attention.self.key.weight\n",
      "encoder.layer.10.attention.self.key.bias\n",
      "encoder.layer.10.attention.self.value.weight\n",
      "encoder.layer.10.attention.self.value.bias\n",
      "encoder.layer.10.attention.output.dense.weight\n",
      "encoder.layer.10.attention.output.dense.bias\n",
      "encoder.layer.10.attention.output.LayerNorm.weight\n",
      "encoder.layer.10.attention.output.LayerNorm.bias\n",
      "encoder.layer.10.intermediate.dense.weight\n",
      "encoder.layer.10.intermediate.dense.bias\n",
      "encoder.layer.10.output.dense.weight\n",
      "encoder.layer.10.output.dense.bias\n",
      "encoder.layer.10.output.LayerNorm.weight\n",
      "encoder.layer.10.output.LayerNorm.bias\n",
      "encoder.layer.11.attention.self.query.weight\n",
      "encoder.layer.11.attention.self.query.bias\n",
      "encoder.layer.11.attention.self.key.weight\n",
      "encoder.layer.11.attention.self.key.bias\n",
      "encoder.layer.11.attention.self.value.weight\n",
      "encoder.layer.11.attention.self.value.bias\n",
      "encoder.layer.11.attention.output.dense.weight\n",
      "encoder.layer.11.attention.output.dense.bias\n",
      "encoder.layer.11.attention.output.LayerNorm.weight\n",
      "encoder.layer.11.attention.output.LayerNorm.bias\n",
      "encoder.layer.11.intermediate.dense.weight\n",
      "encoder.layer.11.intermediate.dense.bias\n",
      "encoder.layer.11.output.dense.weight\n",
      "encoder.layer.11.output.dense.bias\n",
      "encoder.layer.11.output.LayerNorm.weight\n",
      "encoder.layer.11.output.LayerNorm.bias\n",
      "encoder.layer.12.attention.self.query.weight\n",
      "encoder.layer.12.attention.self.query.bias\n",
      "encoder.layer.12.attention.self.key.weight\n",
      "encoder.layer.12.attention.self.key.bias\n",
      "encoder.layer.12.attention.self.value.weight\n",
      "encoder.layer.12.attention.self.value.bias\n",
      "encoder.layer.12.attention.output.dense.weight\n",
      "encoder.layer.12.attention.output.dense.bias\n",
      "encoder.layer.12.attention.output.LayerNorm.weight\n",
      "encoder.layer.12.attention.output.LayerNorm.bias\n",
      "encoder.layer.12.intermediate.dense.weight\n",
      "encoder.layer.12.intermediate.dense.bias\n",
      "encoder.layer.12.output.dense.weight\n",
      "encoder.layer.12.output.dense.bias\n",
      "encoder.layer.12.output.LayerNorm.weight\n",
      "encoder.layer.12.output.LayerNorm.bias\n",
      "encoder.layer.13.attention.self.query.weight\n",
      "encoder.layer.13.attention.self.query.bias\n",
      "encoder.layer.13.attention.self.key.weight\n",
      "encoder.layer.13.attention.self.key.bias\n",
      "encoder.layer.13.attention.self.value.weight\n",
      "encoder.layer.13.attention.self.value.bias\n",
      "encoder.layer.13.attention.output.dense.weight\n",
      "encoder.layer.13.attention.output.dense.bias\n",
      "encoder.layer.13.attention.output.LayerNorm.weight\n",
      "encoder.layer.13.attention.output.LayerNorm.bias\n",
      "encoder.layer.13.intermediate.dense.weight\n",
      "encoder.layer.13.intermediate.dense.bias\n",
      "encoder.layer.13.output.dense.weight\n",
      "encoder.layer.13.output.dense.bias\n",
      "encoder.layer.13.output.LayerNorm.weight\n",
      "encoder.layer.13.output.LayerNorm.bias\n",
      "encoder.layer.14.attention.self.query.weight\n",
      "encoder.layer.14.attention.self.query.bias\n",
      "encoder.layer.14.attention.self.key.weight\n",
      "encoder.layer.14.attention.self.key.bias\n",
      "encoder.layer.14.attention.self.value.weight\n",
      "encoder.layer.14.attention.self.value.bias\n",
      "encoder.layer.14.attention.output.dense.weight\n",
      "encoder.layer.14.attention.output.dense.bias\n",
      "encoder.layer.14.attention.output.LayerNorm.weight\n",
      "encoder.layer.14.attention.output.LayerNorm.bias\n",
      "encoder.layer.14.intermediate.dense.weight\n",
      "encoder.layer.14.intermediate.dense.bias\n",
      "encoder.layer.14.output.dense.weight\n",
      "encoder.layer.14.output.dense.bias\n",
      "encoder.layer.14.output.LayerNorm.weight\n",
      "encoder.layer.14.output.LayerNorm.bias\n",
      "encoder.layer.15.attention.self.query.weight\n",
      "encoder.layer.15.attention.self.query.bias\n",
      "encoder.layer.15.attention.self.key.weight\n",
      "encoder.layer.15.attention.self.key.bias\n",
      "encoder.layer.15.attention.self.value.weight\n",
      "encoder.layer.15.attention.self.value.bias\n",
      "encoder.layer.15.attention.output.dense.weight\n",
      "encoder.layer.15.attention.output.dense.bias\n",
      "encoder.layer.15.attention.output.LayerNorm.weight\n",
      "encoder.layer.15.attention.output.LayerNorm.bias\n",
      "encoder.layer.15.intermediate.dense.weight\n",
      "encoder.layer.15.intermediate.dense.bias\n",
      "encoder.layer.15.output.dense.weight\n",
      "encoder.layer.15.output.dense.bias\n",
      "encoder.layer.15.output.LayerNorm.weight\n",
      "encoder.layer.15.output.LayerNorm.bias\n",
      "encoder.layer.16.attention.self.query.weight\n",
      "encoder.layer.16.attention.self.query.bias\n",
      "encoder.layer.16.attention.self.key.weight\n",
      "encoder.layer.16.attention.self.key.bias\n",
      "encoder.layer.16.attention.self.value.weight\n",
      "encoder.layer.16.attention.self.value.bias\n",
      "encoder.layer.16.attention.output.dense.weight\n",
      "encoder.layer.16.attention.output.dense.bias\n",
      "encoder.layer.16.attention.output.LayerNorm.weight\n",
      "encoder.layer.16.attention.output.LayerNorm.bias\n",
      "encoder.layer.16.intermediate.dense.weight\n",
      "encoder.layer.16.intermediate.dense.bias\n",
      "encoder.layer.16.output.dense.weight\n",
      "encoder.layer.16.output.dense.bias\n",
      "encoder.layer.16.output.LayerNorm.weight\n",
      "encoder.layer.16.output.LayerNorm.bias\n",
      "encoder.layer.17.attention.self.query.weight\n",
      "encoder.layer.17.attention.self.query.bias\n",
      "encoder.layer.17.attention.self.key.weight\n",
      "encoder.layer.17.attention.self.key.bias\n",
      "encoder.layer.17.attention.self.value.weight\n",
      "encoder.layer.17.attention.self.value.bias\n",
      "encoder.layer.17.attention.output.dense.weight\n",
      "encoder.layer.17.attention.output.dense.bias\n",
      "encoder.layer.17.attention.output.LayerNorm.weight\n",
      "encoder.layer.17.attention.output.LayerNorm.bias\n",
      "encoder.layer.17.intermediate.dense.weight\n",
      "encoder.layer.17.intermediate.dense.bias\n",
      "encoder.layer.17.output.dense.weight\n",
      "encoder.layer.17.output.dense.bias\n",
      "encoder.layer.17.output.LayerNorm.weight\n",
      "encoder.layer.17.output.LayerNorm.bias\n",
      "encoder.layer.18.attention.self.query.weight\n",
      "encoder.layer.18.attention.self.query.bias\n",
      "encoder.layer.18.attention.self.key.weight\n",
      "encoder.layer.18.attention.self.key.bias\n",
      "encoder.layer.18.attention.self.value.weight\n",
      "encoder.layer.18.attention.self.value.bias\n",
      "encoder.layer.18.attention.output.dense.weight\n",
      "encoder.layer.18.attention.output.dense.bias\n",
      "encoder.layer.18.attention.output.LayerNorm.weight\n",
      "encoder.layer.18.attention.output.LayerNorm.bias\n",
      "encoder.layer.18.intermediate.dense.weight\n",
      "encoder.layer.18.intermediate.dense.bias\n",
      "encoder.layer.18.output.dense.weight\n",
      "encoder.layer.18.output.dense.bias\n",
      "encoder.layer.18.output.LayerNorm.weight\n",
      "encoder.layer.18.output.LayerNorm.bias\n",
      "encoder.layer.19.attention.self.query.weight\n",
      "encoder.layer.19.attention.self.query.bias\n",
      "encoder.layer.19.attention.self.key.weight\n",
      "encoder.layer.19.attention.self.key.bias\n",
      "encoder.layer.19.attention.self.value.weight\n",
      "encoder.layer.19.attention.self.value.bias\n",
      "encoder.layer.19.attention.output.dense.weight\n",
      "encoder.layer.19.attention.output.dense.bias\n",
      "encoder.layer.19.attention.output.LayerNorm.weight\n",
      "encoder.layer.19.attention.output.LayerNorm.bias\n",
      "encoder.layer.19.intermediate.dense.weight\n",
      "encoder.layer.19.intermediate.dense.bias\n",
      "encoder.layer.19.output.dense.weight\n",
      "encoder.layer.19.output.dense.bias\n",
      "encoder.layer.19.output.LayerNorm.weight\n",
      "encoder.layer.19.output.LayerNorm.bias\n",
      "encoder.layer.20.attention.self.query.weight\n",
      "encoder.layer.20.attention.self.query.bias\n",
      "encoder.layer.20.attention.self.key.weight\n",
      "encoder.layer.20.attention.self.key.bias\n",
      "encoder.layer.20.attention.self.value.weight\n",
      "encoder.layer.20.attention.self.value.bias\n",
      "encoder.layer.20.attention.output.dense.weight\n",
      "encoder.layer.20.attention.output.dense.bias\n",
      "encoder.layer.20.attention.output.LayerNorm.weight\n",
      "encoder.layer.20.attention.output.LayerNorm.bias\n",
      "encoder.layer.20.intermediate.dense.weight\n",
      "encoder.layer.20.intermediate.dense.bias\n",
      "encoder.layer.20.output.dense.weight\n",
      "encoder.layer.20.output.dense.bias\n",
      "encoder.layer.20.output.LayerNorm.weight\n",
      "encoder.layer.20.output.LayerNorm.bias\n",
      "encoder.layer.21.attention.self.query.weight\n",
      "encoder.layer.21.attention.self.query.bias\n",
      "encoder.layer.21.attention.self.key.weight\n",
      "encoder.layer.21.attention.self.key.bias\n",
      "encoder.layer.21.attention.self.value.weight\n",
      "encoder.layer.21.attention.self.value.bias\n",
      "encoder.layer.21.attention.output.dense.weight\n",
      "encoder.layer.21.attention.output.dense.bias\n",
      "encoder.layer.21.attention.output.LayerNorm.weight\n",
      "encoder.layer.21.attention.output.LayerNorm.bias\n",
      "encoder.layer.21.intermediate.dense.weight\n",
      "encoder.layer.21.intermediate.dense.bias\n",
      "encoder.layer.21.output.dense.weight\n",
      "encoder.layer.21.output.dense.bias\n",
      "encoder.layer.21.output.LayerNorm.weight\n",
      "encoder.layer.21.output.LayerNorm.bias\n",
      "encoder.layer.22.attention.self.query.weight\n",
      "encoder.layer.22.attention.self.query.bias\n",
      "encoder.layer.22.attention.self.key.weight\n",
      "encoder.layer.22.attention.self.key.bias\n",
      "encoder.layer.22.attention.self.value.weight\n",
      "encoder.layer.22.attention.self.value.bias\n",
      "encoder.layer.22.attention.output.dense.weight\n",
      "encoder.layer.22.attention.output.dense.bias\n",
      "encoder.layer.22.attention.output.LayerNorm.weight\n",
      "encoder.layer.22.attention.output.LayerNorm.bias\n",
      "encoder.layer.22.intermediate.dense.weight\n",
      "encoder.layer.22.intermediate.dense.bias\n",
      "encoder.layer.22.output.dense.weight\n",
      "encoder.layer.22.output.dense.bias\n",
      "encoder.layer.22.output.LayerNorm.weight\n",
      "encoder.layer.22.output.LayerNorm.bias\n",
      "encoder.layer.23.attention.self.query.weight\n",
      "encoder.layer.23.attention.self.query.bias\n",
      "encoder.layer.23.attention.self.key.weight\n",
      "encoder.layer.23.attention.self.key.bias\n",
      "encoder.layer.23.attention.self.value.weight\n",
      "encoder.layer.23.attention.self.value.bias\n",
      "encoder.layer.23.attention.output.dense.weight\n",
      "encoder.layer.23.attention.output.dense.bias\n",
      "encoder.layer.23.attention.output.LayerNorm.weight\n",
      "encoder.layer.23.attention.output.LayerNorm.bias\n",
      "encoder.layer.23.intermediate.dense.weight\n",
      "encoder.layer.23.intermediate.dense.bias\n",
      "encoder.layer.23.output.dense.weight\n",
      "encoder.layer.23.output.dense.bias\n",
      "encoder.layer.23.output.LayerNorm.weight\n",
      "encoder.layer.23.output.LayerNorm.bias\n",
      "pooler.dense.weight\n",
      "pooler.dense.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.bert.named_parameters():                \n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41d0859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_tagging_pred_n_true(preds, y, tag_pad_idx, org_shap, fist_tokens, FT_TAGS):\n",
    "\n",
    "    max_preds_p = preds.argmax(dim = 1, keepdim = True).view(org_shap)\n",
    "    y_p = y.view(org_shap)\n",
    "    fist_tokens_p = fist_tokens.view(org_shap)\n",
    "   \n",
    "    preds_list = []\n",
    "    true_list = []\n",
    "    for i in range(len(y_p)):\n",
    "        seq_pred = []\n",
    "        seq_true = []\n",
    "        for j in range(len(y_p[i])):\n",
    "\n",
    "            if y_p[i][j].item() != tag_pad_idx and fist_tokens_p[i][j] == 1:\n",
    "                seq_pred.append(max_preds_p[i][j].item()-1)\n",
    "                seq_true.append(y_p[i][j].item()-1)\n",
    "\n",
    "        preds_list.append(seq_pred)\n",
    "        true_list.append(seq_true)\n",
    "\n",
    "    return preds_list, true_list\n",
    "\n",
    "def obtain_classification_pred_n_true(preds, y):\n",
    "\n",
    "    max_preds = preds.argmax(dim = 1)\n",
    "   \n",
    "    preds_list = []\n",
    "    true_list = []\n",
    "    for i in range(len(y)):\n",
    "        preds_list.append(max_preds[i].item()-1)\n",
    "        true_list.append(y[i].item()-1)\n",
    "\n",
    "    return preds_list, true_list\n",
    "\n",
    "def f1_score_tag(preds_list, true_list):\n",
    "    tp,tn,fp,fn = 0,0,0,0\n",
    "    for i in range(len(true_list)):\n",
    "        for j in range(len(true_list[i])):\n",
    "            if true_list[i][j] == 1:\n",
    "                if preds_list[i][j] == 1:\n",
    "                    tp+=1\n",
    "                elif preds_list[i][j] == 0:\n",
    "                    fn += 1\n",
    "            elif true_list[i][j] == 0:\n",
    "                if preds_list[i][j] == 0:\n",
    "                    tn += 1\n",
    "                elif preds_list[i][j] == 1:\n",
    "                    fp += 1\n",
    "               \n",
    "    recall = tp/(tp+fn+1e-9)\n",
    "    precision = tp/(tp+fp+1e-9)\n",
    "    f1 = 2*recall*precision/(recall+precision+1e-9)\n",
    "    acc = (tp+tn)/(tp+tn+fp+fn+1e-9)\n",
    "    return round(precision,4), round(recall,4), round(f1,4), round(acc,4)\n",
    "\n",
    "def f1_score_cls(preds_list, true_list):\n",
    "    tp,tn,fp,fn = 0,0,0,0\n",
    "    for i in range(len(true_list)):\n",
    "        if true_list[i] == 1:\n",
    "            if preds_list[i] == 1:\n",
    "                tp+=1\n",
    "            elif preds_list[i] == 0:\n",
    "                fn += 1\n",
    "        elif true_list[i] == 0:\n",
    "            if preds_list[i] == 0:\n",
    "                tn += 1\n",
    "            elif preds_list[i] == 1:\n",
    "                fp += 1\n",
    "               \n",
    "    recall = tp/(tp+fn+1e-9)\n",
    "    precision = tp/(tp+fp+1e-9)\n",
    "    f1 = 2*recall*precision/(recall+precision+1e-9)\n",
    "    acc = (tp+tn)/(tp+tn+fp+fn+1e-9)\n",
    "    return round(precision,4), round(recall,4), round(f1,4), round(acc,4)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion_label, criterion_pos, tag_pad_idx, tag_loss_weight, cls_loss_weight):\n",
    "    \n",
    "    loss_label1 = 0\n",
    "    loss_label2 = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    pred_label1_list=[]\n",
    "    true_label1_list=[]\n",
    "    pred_label2_list=[]\n",
    "    true_label2_list=[]\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        org_label1_shap = batch.label1.size() # [batch size, seq len]\n",
    "        org_label2_shap = batch.label2.size() # [batch size, 1]\n",
    "        \n",
    "        token_idx_ = batch.token_id # [batch size, seq len]\n",
    "        mask_ = batch.mask # [batch size, seq len]\n",
    "        ft_tags = batch.first_token.view(-1) # [batch size * seq len]\n",
    "        tag_true = batch.label1.view(-1) # [batch size * seq len]\n",
    "        cls_true = batch.label2.view(-1) # [batch size]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # tag_pred:[batch size, seq len, label1 dim]\n",
    "        # cls_pred:[batch size, label2 dim]\n",
    "        tag_pred, cls_pred = model(token_idx_,mask_)\n",
    "\n",
    "        tag_pred = tag_pred.view(-1, tag_pred.shape[-1]) # [batch size * seq len, label1 dim]\n",
    "\n",
    "        loss_tag = criterion_tag(tag_pred, tag_true)\n",
    "        loss_cls = criterion_cls(cls_pred, cls_true)\n",
    "        \n",
    "        loss_label1 += loss_tag.item()\n",
    "        loss_label2 += loss_cls.item()\n",
    "        \n",
    "        # loss_tag is larger than loss_cls, so the overall loss is weighted summed up.\n",
    "        # if loss_cls > loss_tag*10:\n",
    "        #     cls_loss_weight=0.1\n",
    "        # elif loss_cls > loss_tag*5:\n",
    "        #     cls_loss_weight=0.2\n",
    "        # elif loss_cls > loss_tag*2:\n",
    "        #     cls_loss_weight=0.5\n",
    "        # elif loss_cls < loss_tag*0.5:\n",
    "        #     cls_loss_weight=2\n",
    "        \n",
    "        # tag_loss_weight=1\n",
    "        # cls_loss_weight=tag_loss_weight*loss_tag/loss_cls\n",
    "        \n",
    "        \n",
    "        loss = tag_loss_weight*loss_tag + cls_loss_weight*loss_cls\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        pred_label1, true_label1 = obtain_tagging_pred_n_true(tag_pred, tag_true, tag_pad_idx, org_label1_shap, ft_tags, FT_TAGS)\n",
    "        pred_label2, true_label2 = obtain_classification_pred_n_true(cls_pred, cls_true)\n",
    "        \n",
    "        \n",
    "        pred_label1_list.extend(pred_label1)\n",
    "        true_label1_list.extend(true_label1)\n",
    "        pred_label2_list.extend(pred_label2)\n",
    "        true_label2_list.extend(true_label2)\n",
    "            \n",
    "            \n",
    "    p_label1, r_label1, f_label1, acc_label1 = f1_score_tag(pred_label1_list, true_label1_list)\n",
    "    p_label2, r_label2, f_label2, acc_label2 = f1_score_cls(pred_label2_list, true_label2_list)\n",
    "        \n",
    "    return round(loss_label1,4), round(loss_label2,4), p_label1, r_label1, f_label1, acc_label1, p_label2, r_label2, f_label2, acc_label2\n",
    "\n",
    "def evaluate(model, iterator, criterion_label, criterion_pos, tag_pad_idx):\n",
    "    \n",
    "    loss_label1 = 0\n",
    "    loss_label2 = 0\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        pred_label1_list=[]\n",
    "        true_label1_list=[]\n",
    "        pred_label2_list=[]\n",
    "        true_label2_list=[]\n",
    "        \n",
    "        for batch in iterator:\n",
    "\n",
    "            org_label1_shap = batch.label1.size() # [batch size, seq len]\n",
    "            org_label2_shap = batch.label2.size() # [batch size, 1]\n",
    "\n",
    "            token_idx_ = batch.token_id # [batch size, seq len]\n",
    "            mask_ = batch.mask # [batch size, seq len]\n",
    "            ft_tags = batch.first_token.view(-1) # [batch size * seq len]\n",
    "            tag_true = batch.label1.view(-1) # [batch size * seq len]\n",
    "            cls_true = batch.label2.view(-1) # [batch size]\n",
    "\n",
    "            # tag_pred:[batch size, seq len, label1 dim]\n",
    "            # cls_pred:[batch size, label2 dim]\n",
    "            tag_pred, cls_pred = model(token_idx_,mask_)\n",
    "\n",
    "            tag_pred = tag_pred.view(-1, tag_pred.shape[-1]) # [batch size * seq len, label1 dim]\n",
    "\n",
    "            loss_tag = criterion_tag(tag_pred, tag_true)\n",
    "            loss_cls = criterion_cls(cls_pred, cls_true)\n",
    "\n",
    "            loss_label1 += loss_tag.item()\n",
    "            loss_label2 += loss_cls.item()\n",
    "\n",
    "            pred_label1, true_label1 = obtain_tagging_pred_n_true(tag_pred, tag_true, tag_pad_idx, org_label1_shap, ft_tags, FT_TAGS)\n",
    "            pred_label2, true_label2 = obtain_classification_pred_n_true(cls_pred, cls_true)\n",
    "            \n",
    "            pred_label1_list.extend(pred_label1)\n",
    "            true_label1_list.extend(true_label1)\n",
    "            pred_label2_list.extend(pred_label2)\n",
    "            true_label2_list.extend(true_label2)\n",
    "            \n",
    "            \n",
    "        p_label1, r_label1, f_label1, acc_label1 = f1_score_tag(pred_label1_list, true_label1_list)\n",
    "        p_label2, r_label2, f_label2, acc_label2 = f1_score_cls(pred_label2_list, true_label2_list)\n",
    "        \n",
    "    return round(loss_label1,4), round(loss_label2,4), p_label1, r_label1, f_label1, acc_label1, p_label2, r_label2, f_label2, acc_label2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b20b0b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os, shutil\n",
    "import datetime\n",
    "\n",
    "OUTPUT_PATH = './checkpoint'\n",
    "OUTPUT_PATH =os.path.join(OUTPUT_PATH, BERT_PATH[2:])\n",
    "\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "\n",
    "\n",
    "label1_loss_train_list=[]\n",
    "label1_loss_validation_list=[]\n",
    "label1_loss_test_list=[]\n",
    "\n",
    "label2_loss_train_list=[]\n",
    "label2_loss_validation_list=[]\n",
    "label2_loss_test_list=[]\n",
    "\n",
    "label1_f1_train_list=[]\n",
    "label1_f1_validation_list=[]\n",
    "label1_f1_test_list=[]\n",
    "\n",
    "label2_f1_train_list=[]\n",
    "label2_f1_validation_list=[]\n",
    "label2_f1_test_list=[]\n",
    "\n",
    "best_label2_f1=-1\n",
    "for epoch in range(N_EPOCHS):\n",
    "    loss_label1_tr, loss_label2_tr, p_label1_tr, r_label1_tr, f_label1_tr, acc_label1_tr, p_label2_tr, r_label2_tr, f_label2_tr, acc_label2_tr = train(model, train_iterator, optimizer, criterion_tag, criterion_cls, TAG_PAD_IDX, TAG_LOSS_WEIGTH, CLS_LOSS_WEIGTH)\n",
    "    loss_label1_va, loss_label2_va, p_label1_va, r_label1_va, f_label1_va, acc_label1_va, p_label2_va, r_label2_va, f_label2_va, acc_label2_va = evaluate(model, valid_iterator, criterion_tag, criterion_cls, TAG_PAD_IDX)\n",
    "    loss_label1_te, loss_label2_te, p_label1_te, r_label1_te, f_label1_te, acc_label1_te, p_label2_te, r_label2_te, f_label2_te, acc_label2_te = evaluate(model, test_iterator, criterion_tag, criterion_cls, TAG_PAD_IDX)\n",
    "    print('EPOCH', epoch)\n",
    "    print('   TRAIN | Label 1 loss:', loss_label1_tr, '; P:', p_label1_tr, '; R:', r_label1_tr, '; F1:', f_label1_tr, '; Acc:', acc_label1_tr)\n",
    "    print('           Label 2 loss:', loss_label2_tr, '; P:', p_label2_tr, '; R:', r_label2_tr, '; F1:', f_label2_tr, '; Acc:', acc_label2_tr)\n",
    "    print('   VALID | Label 1 loss:', loss_label1_va, '; P:', p_label1_va, '; R:', r_label1_va, '; F1:', f_label1_va, '; Acc:', acc_label1_va)\n",
    "    print('           Label 2 loss:', loss_label2_va, '; P:', p_label2_va, '; R:', r_label2_va, '; F1:', f_label2_va, '; Acc:', acc_label2_va)\n",
    "    print('    TEST | Label 1 loss:', loss_label1_te, '; P:', p_label1_te, '; R:', r_label1_te, '; F1:', f_label1_te, '; Acc:', acc_label1_te)\n",
    "    print('           Label 2 loss:', loss_label2_te, '; P:', p_label2_te, '; R:', r_label2_te, '; F1:', f_label2_te, '; Acc:', acc_label2_te)\n",
    "    print(datetime.datetime.now())\n",
    "    \n",
    "    label1_loss_train_list.append(loss_label1_tr)\n",
    "    label1_loss_validation_list.append(loss_label1_va)\n",
    "    label1_loss_test_list.append(loss_label1_te)\n",
    "    \n",
    "    label2_loss_train_list.append(loss_label2_tr)\n",
    "    label2_loss_validation_list.append(loss_label2_va)\n",
    "    label2_loss_test_list.append(loss_label2_te)\n",
    "    \n",
    "    label1_f1_train_list.append(f_label1_tr)\n",
    "    label1_f1_validation_list.append(f_label1_va)\n",
    "    label1_f1_test_list.append(f_label1_te)\n",
    "    \n",
    "    label2_f1_train_list.append(f_label2_tr)\n",
    "    label2_f1_validation_list.append(f_label2_va)\n",
    "    label2_f1_test_list.append(f_label2_te)\n",
    "    \n",
    "    if f_label2_va>best_label2_f1:\n",
    "        best_label2_f1=f_label2_va\n",
    "        output_dir = os.path.join(OUTPUT_PATH, 'checkpoint_epoch{}.pt'.format(epoch))\n",
    "        # clear the content of folder\n",
    "        for files in os.listdir(OUTPUT_PATH):\n",
    "            path = os.path.join(OUTPUT_PATH, files)\n",
    "            try:\n",
    "                shutil.rmtree(path)\n",
    "            except OSError:\n",
    "                os.remove(path)\n",
    "        # if not os.path.exists(output_dir):\n",
    "        #     os.makedirs(output_dir)\n",
    "        torch.save(model.state_dict(), output_dir)\n",
    "\n",
    "    \n",
    "    \n",
    "label1_loss_df = pd.DataFrame(\n",
    "    {'label1_loss_train': label1_loss_train_list,\n",
    "     'label1_loss_validation': label1_loss_validation_list,\n",
    "     'label1_loss_test': label1_loss_test_list\n",
    "    })\n",
    "label2_loss_df = pd.DataFrame(\n",
    "    {'label2_loss_train': label2_loss_train_list,\n",
    "     'label2_loss_validation': label2_loss_validation_list,\n",
    "     'label2_loss_test': label2_loss_test_list\n",
    "    })\n",
    "label1_f1_df = pd.DataFrame(\n",
    "    {'label1_f1_train': label1_f1_train_list,\n",
    "     'label1_f1_validation': label1_f1_validation_list,\n",
    "     'label1_f1_test': label1_f1_test_list\n",
    "    })\n",
    "label2_f1_df = pd.DataFrame(\n",
    "    {'label2_f1_train': label2_f1_train_list,\n",
    "     'label2_f1_validation': label2_f1_validation_list,\n",
    "     'label2_f1_test': label2_f1_test_list\n",
    "    })\n",
    "label1_loss_df.plot()\n",
    "label2_loss_df.plot()\n",
    "label1_f1_df.plot()\n",
    "label2_f1_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5918fbda-c096-42f6-b749-32b7c14a9176",
   "metadata": {},
   "outputs": [],
   "source": [
    "label1_loss_df.plot(title=\"label1 losses\",xlabel=\"epochs\",ylabel=\"loss\")\n",
    "label2_loss_df.plot(title=\"label2 losses\",xlabel=\"epochs\",ylabel=\"loss\")\n",
    "label1_f1_df.plot(title=\"label1 f1\",xlabel=\"epochs\",ylabel=\"f1\")\n",
    "label2_f1_df.plot(title=\"label2 f1\",xlabel=\"epochs\",ylabel=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6591fe4a-1a14-41c6-a43d-563c0b1d6af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(label2_f1_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08e1173-eb47-4381-bb91-5e8822d0b9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(label2_f1_validation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2b8ddc-2236-4252-b626-e33e9a136bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
